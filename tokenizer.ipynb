{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading raw text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (\"the-verdict.txt\",\"r\",encoding=\"utf-8\") as file:\n",
    "    raw_text=file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "preprocessed= re.split(r'([,.:?_!\"()\\']|--|\\s)',raw_text)\n",
    "result= [items.strip() for items in preprocessed if items.split()]\n",
    "allwords=sorted(set(result))\n",
    "allwords.extend([\"<|unk|>\",\"<|endoftext|>\"])\n",
    "vocab= {word:index for index,word in enumerate(allwords)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementation of simple tokenizer -- For understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleTokenizer1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int=vocab\n",
    "        self.int_to_str={int:str for str,int in vocab.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        _preprocessed= re.split(r'([,.:?_!\"()\\']|--|\\s)',text)\n",
    "        _result= [items.strip() for items in _preprocessed if items.split()]\n",
    "        return [self.str_to_int[items] for items in _result]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        text= \" \".join([self.int_to_str[items] for items in tokens])\n",
    "        text= re.sub(r'\\s([,.:?_!\"()\\']|--|\\s)',r'\\1',text)\n",
    "        return text          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1014, 58, 39]\n",
      "['thought', 'Jack', 'Gisburn']\n"
     ]
    }
   ],
   "source": [
    "tokenizer= simpleTokenizer1(vocab)\n",
    "text=\"thought Jack Gisburn\"\n",
    "encoded= tokenizer.encode(text)\n",
    "print(encoded)\n",
    "decode= tokenizer.decode(encoded)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleTokenizer2:\n",
    "    def __init__(self, vocab):\n",
    "        self.int_to_str={str:int for int,str in vocab.items()}\n",
    "        self.str_to_int=vocab\n",
    "        \n",
    "    def encode(self, text):\n",
    "        preprocessed= re.split(r'([.,?!_!\"()\\']|--|\\s)',text)\n",
    "        _preprocessed= [items.strip() for items in preprocessed if items.split()]\n",
    "        encoded_text= [item if item in self.str_to_int \n",
    "                       else \"<|unk|>\" for item in _preprocessed]\n",
    "        return [self.str_to_int[items] for items in encoded_text]\n",
    "    \n",
    "    def decode(self, tokens):\n",
    "        text= \" \".join([self.int_to_str[items] for items in tokens])\n",
    "        text= re.sub(r'\\s([,.:?_!\"()\\']|--|\\s)',r'\\1',text)\n",
    "        return text \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1143, 705, 1143, 590, 1143]\n",
      "['<|unk|>', 'my', '<|unk|>', 'is', '<|unk|>']\n"
     ]
    }
   ],
   "source": [
    "tokenizer2= simpleTokenizer2(vocab)\n",
    "text=\"Hello my name is Anant\"\n",
    "encoded= tokenizer2.encode(text)\n",
    "print(encoded)\n",
    "decode= tokenizer2.decode(encoded)\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-Pair Encoding --> A type of subword encoding technique used in GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "tokenizer= tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing dataset, Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset_V1:\n",
    "    \n",
    "    def __init__(self, text, tokenizer, maximum_length, stride):\n",
    "        \n",
    "        self.input_ids=[]\n",
    "        self.target_ids=[] \n",
    "        \n",
    "        token_ids= tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "        \n",
    "        for i in range(0, len(token_ids)-maximum_length, stride):\n",
    "            input_tokens = token_ids[i:i+maximum_length]\n",
    "            output_tokens= token_ids[i+1:i+maximum_length+1]\n",
    "            \n",
    "            self.input_ids.append(input_tokens)\n",
    "            self.target_ids.append(output_tokens)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Dataloader_V1(text, batch_size=4, maximum_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \n",
    "    tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset= GPTDataset_V1(text, tokenizer, maximum_length, stride)\n",
    "\n",
    "    dataloader= DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers)\n",
    "    \n",
    "    return dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
